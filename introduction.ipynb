{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QKV Attentionについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attnetionは上式のように定義されている.\n",
    "\n",
    "これはさまざまな解釈ができるが僕の好みはトークン(特徴量ベクトル)の集合をその集合内の類似性を考慮した新しい集合に進化させるためのものだと考えることである.\n",
    "\n",
    "データセットが与えられたとき, そのデータセット内の類似性を求めようと思ったとき, もっとも素朴なやり方はデータセット同士の行列積を求めることである.\n",
    "これはデータ同士の内積はcos類似度に対応することからのアナロジー取れ, またグラム行列からも理解できる.\n",
    "\n",
    "Attentionはその考えたデータセットを一列に並べて, (データ数(=バッチサイズ)*特徴量)のサイズの大きさの行列とし, そしてデータセットの類似性を行列積として計算し, さらにそれをデータセットに適用して進化させる. \n",
    "Attentionはそれを\n",
    "1. まずデータセットをQuery(類似度用), Key(類似度用), Value(出力用)の3つに加工する\n",
    "2. QueryとKeyの行列積を求め, 特徴量次元を考慮して正規化を行うことで類似度を作る(Attention weight)\n",
    "3. QueryとKeyで作った類似度を使って, Valueに適用させて, 新しいデータセットを作る\n",
    "\n",
    "なぜQueryとKeyの二つがあるかというといろいろ解釈があるが, 自分同士の類似度だけじゃなくて別のデータセットとの類似度を求めるための自由度を持たせたと考えることが僕の好みである.\n",
    "\n",
    "注意として, Attention自体はほぼ行列積そのものであり, パラメータを持たない.\n",
    "Attentionのパラメータと呼ばれるものはQuery, Key, Valueを作るときにデータセットにLinearを通すが, このLinearのパラメータのことだ.\n",
    "(MultiHeadAttentionのときはこういう呼ばれ方がされることがある)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def QKVattention(Query, Key, Value, attention_QKV_dim):\n",
    "    attention_score = torch.matmul(Query, Key.transpose(-2, -1))/attention_QKV_dim\n",
    "    attention_weight = F.softmax(attention_score, dim=-1)\n",
    "    attention = torch.matmul(attention_weight, Value)\n",
    "    return attention\n",
    "\n",
    "class multihead_attention(nn.Module):\n",
    "    def __init__(self, Q_tensor_dim, K_tensor_dim, V_tensor_dim, attention_QKV_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.Q_tensor_dim = Q_tensor_dim\n",
    "        self.K_tensor_dim = K_tensor_dim\n",
    "        self.V_tensor_dim = V_tensor_dim\n",
    "        self.attention_QKV_dim = attention_QKV_dim\n",
    "        self.linear_Q = nn.Linear(Q_tensor_dim, attention_QKV_dim)\n",
    "        self.linear_K = nn.Linear(K_tensor_dim, attention_QKV_dim)\n",
    "        self.linear_V = nn.Linear(V_tensor_dim, attention_QKV_dim)\n",
    "\n",
    "    def forward(self, Q_tensor, K_tensor, V_tensor):\n",
    "        Q = self.linear_Q(Q_tensor)\n",
    "        K = self.linear_K(K_tensor)\n",
    "        V = self.linear_V(V_tensor)\n",
    "        attention_matrix = QKVattention(Q, K, V, self.attention_QKV_dim)\n",
    "        return attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1742, 0.2942, 0.2527, 0.8415, 0.9068],\n",
      "        [0.0389, 0.4320, 0.1214, 0.8945, 0.4234],\n",
      "        [0.4047, 0.2564, 0.5341, 0.1083, 0.9343],\n",
      "        [0.6210, 0.2592, 0.4510, 0.9350, 0.7226]])\n"
     ]
    }
   ],
   "source": [
    "Q_tensor = torch.rand(4, 5)\n",
    "K_tensor = torch.rand(4, 5)\n",
    "V_tensor = torch.rand(4, 5)\n",
    "print(Q_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1745, 0.1961, 0.5204],\n",
      "        [0.1746, 0.1960, 0.5204],\n",
      "        [0.1749, 0.1961, 0.5206],\n",
      "        [0.1746, 0.1956, 0.5201]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention = multihead_attention(5, 5, 5, 3)\n",
    "print(attention(Q_tensor, K_tensor, V_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(nn.Module):\n",
    "    def __init__(self, attention_QKV_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.attn_layer = multihead_attention(attention_QKV_dim, attention_QKV_dim, attention_QKV_dim, attention_QKV_dim)\n",
    "        self.MLP_layer = nn.Sequential(nn.Linear(attention_QKV_dim, attention_QKV_dim), nn.ReLU(), nn.Linear(attention_QKV_dim, attention_QKV_dim))\n",
    "        \n",
    "    def forward(self, Q_tensor, K_tensor, V_tensor):\n",
    "        attention_matrix = self.attn_layer(Q_tensor, K_tensor, V_tensor)\n",
    "        after_mlp = self.MLP_layer(attention_matrix)\n",
    "        return after_mlp\n",
    "        \n",
    "class GenerativePretrainedTransformer(nn.Module):\n",
    "    def __init__(self, Q_tensor_dim, K_tensor_dim, V_tensor_dim, attention_QKV_dim, transformer_layer_number):\n",
    "        super().__init__()\n",
    "        self.initial_attention = multihead_attention(Q_tensor_dim, K_tensor_dim, V_tensor_dim, attention_QKV_dim)\n",
    "        self.multi_layer_transformer_blocks = nn.ModuleList([transformer_block(attention_QKV_dim) for i in range(transformer_layer_number)])\n",
    "        \n",
    "    def forward(self, Q_tensor, K_tensor, V_tensor):\n",
    "        initial_attn = self.initial_attention(Q_tensor, K_tensor, V_tensor)\n",
    "        for idx, block in enumerate(self.multi_layer_transformer_blocks):\n",
    "            if idx == 0:\n",
    "                out = block(initial_attn, initial_attn, initial_attn)\n",
    "            else:\n",
    "                out = block(out, out, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
